{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# download corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 터미널에서 사용할 것은 apt로 설치\n",
    "#!apt install -y curl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls\n",
    "#!mkdir data\n",
    "\n",
    "# small wiki corpus\n",
    "#!curl -c ./data -s -L \"https://drive.google.com/uc?export=download&id=1zib1GI8Q5wV08TgYBa2GagqNh4jyfXZz\" > /dev/null\n",
    "#!curl -Lb ./data \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1zib1GI8Q5wV08TgYBa2GagqNh4jyfXZz\" -o data/wiki_20190620_small.txt\n",
    "\n",
    "# all kor wiki corpus\n",
    "#!curl -c ./data -s -L \"https://drive.google.com/uc?export=download&id=1_F5fziHjUM-jKr5Pwcx1we6g_J2o70kZ\" > /dev/null\n",
    "#!curl -Lb ./data \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1_F5fziHjUM-jKr5Pwcx1we6g_J2o70kZ\" -o data/wiki_20190620.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate tokenizer(generate vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir custom_tokenizers\n",
    "\n",
    "# tokenizer models: bpe, unigram, wordlevel, wordpiece\n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['custom_tokenizers/wordpiece_tokenizer-vocab.txt']"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# init\n",
    "wp_tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,           # 공백 제거\n",
    "    handle_chinese_chars=True, # 한자는 하나가 한 토큰\n",
    "    strip_accents=False,      # if True, [StripAccents] -> [Strip, Accents]\n",
    "    lowercase=False,           # if True, LowerCase -> lowercase\n",
    ")\n",
    "\n",
    "# train\n",
    "wp_tokenizer.train(\n",
    "    files=\"/opt/ml/other/BERT_pretrain/data/wiki_20190620_small.txt\",\n",
    "    vocab_size=20000,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    wordpieces_prefix=\"##\",\n",
    ")\n",
    "\n",
    "# save\n",
    "wp_tokenizer.save_model(\"custom_tokenizers\", \"wordpiece_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['[UNK]', '[MASK]', '중', '##기의', '무신', '##이다', '.']\n[1, 4, 755, 2604, 13161, 1895, 16]\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "text = \"뷁은 [MASK] 중기의 무신이다.\"\n",
    "tokenized_text = wp_tokenizer.encode(text)\n",
    "print(tokenized_text.tokens)\n",
    "print(tokenized_text.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create tokenizer (load vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertConfig, BertForPreTraining, BertTokenizerFast\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['[UNK]', '[', 'ma', '##s', '##k', ']', '중', '##기의', '무신', '##이다', '.']\n"
     ]
    }
   ],
   "source": [
    "# custom tokenizer\n",
    "tokenizer = BertTokenizerFast(\n",
    "    vocab_file=\"/opt/ml/other/BERT_pretrain/custom_tokenizers/wordpiece_tokenizer-vocab.txt\",\n",
    "    max_len=128,\n",
    "    strip_accents=False,\n",
    "    lowercase=False,\n",
    ")\n",
    "\n",
    "# check\n",
    "print(tokenizer.tokenize(\"뷁은 [MASK] 중기의 무신이다.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['[UNK]', '[MASK]', '중', '##기의', '무신', '##이다', '.']\n"
     ]
    }
   ],
   "source": [
    "# add [MASK] token\n",
    "tokenizer.add_special_tokens({'mask_token': '[MASK]'})\n",
    "\n",
    "# check\n",
    "print(tokenizer.tokenize(\"뷁은 [MASK] 중기의 무신이다.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101720098"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertConfig, BertForPreTraining\n",
    "\n",
    "# customize default BERT config\n",
    "# https://huggingface.co/transformers/model_doc/bert.html#bertconfig\n",
    "config = BertConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_position_embeddings=tokenizer.model_max_length, # 최대 token 개수. (BERT default=512)\n",
    "    #type_vocab_size=2, # token type id 개수 (BERT는 segmentA, segmentB로 2종류)\n",
    "    #position_embedding_type=\"absolute\"\n",
    ")\n",
    "\n",
    "# init model\n",
    "# for pretraining oneself, use ModelForPreTraining(config)\n",
    "model = BertForPreTraining(config=config)\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from time import time\n",
    "from filelock import FileLock\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers.utils import logging\n",
    "\n",
    "logger = logging.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDatasetForNextSentencePrediction(Dataset):\n",
    "    \"\"\"\n",
    "    This will be superseded by a framework-agnostic approach soon.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        file_path,\n",
    "        block_size,\n",
    "        short_seq_prob=0.1,\n",
    "        nsp_prob=0.5,\n",
    "        overwrite_cache=False,\n",
    "    ):\n",
    "        assert os.path.isfile(file_path), f\"Input file path {file_path} not found\"\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size - tokenizer.num_special_tokens_to_add(pair=True)\n",
    "        self.short_seq_prob = short_seq_prob\n",
    "        self.nsp_prob = nsp_prob\n",
    "\n",
    "        # 학습 데이터 caching\n",
    "        directory, filename = os.path.split(file_path)\n",
    "        cached_features_file = os.path.join(\n",
    "            directory,\n",
    "            \"cached_nsp_{}_{}_{}\".format(\n",
    "                tokenizer.__class__.__name__,\n",
    "                str(block_size),\n",
    "                filename,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        lock_path = cached_features_file + \".lock\"\n",
    "\n",
    "        # Input file format:\n",
    "        # (1) One sentence per line.\n",
    "        # - Must a sentence, not paragraphs or so.\n",
    "        # - Because the task is \"next sentence prediction\".\n",
    "        # (2) Blank lines between documents.\n",
    "        # - Document boundaries are needed.\n",
    "        # - So that the \"next sentence prediction\" task doesn't span between documents.\n",
    "        #\n",
    "        # Example:\n",
    "        # I am very happy.\n",
    "        # Here is the second sentence.\n",
    "        #\n",
    "        # A new document.\n",
    "\n",
    "        with FileLock(lock_path):\n",
    "            # cached data가 존재하면, dataset을 생성할 필요없이 그대로 사용\n",
    "            if os.path.exists(cached_features_file) and not overwrite_cache:\n",
    "                start = time()\n",
    "                with open(cached_features_file, \"rb\") as handle:\n",
    "                    self.examples = pickle.load(handle)\n",
    "                logger.info(\n",
    "                    f\"Loading features from cached file {cached_features_file}\"\n",
    "                    f\"[took {time()-start: .3f} sec]\"\n",
    "                )\n",
    "            # 그렇지 않으면, dataset을 만들어서 사용\n",
    "            else:\n",
    "                # corpus를 load하여 document 별로 grouping(형식화)\n",
    "                logger.info(f\"Creating features from dataset file at {directory}\")\n",
    "                self.documents = [[]]\n",
    "                with open(file_path, encoding=\"utf-8\") as f:\n",
    "                    while True:\n",
    "                        line = f.readline()\n",
    "                        # 종료 조건: 더 이상 line이 없으면 break\n",
    "                        if not line:\n",
    "                            break\n",
    "                        \n",
    "                        # 메인 내용\n",
    "                        # blank line이 나오면 line들을 document로 취합\n",
    "                        # black line은 strip했을 때, not line\n",
    "                        line = line.strip()\n",
    "                        if not line and len(self.documents[-1]) != 0:\n",
    "                            self.documents.append([])\n",
    "                        tokens = tokenizer.tokenize(line)\n",
    "                        tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "                        if tokens:\n",
    "                            self.documents[-1].append(tokens)\n",
    "                \n",
    "                # documents로 examples 생성(학습에 맞는 데이터로 변형)\n",
    "                logger.info(f\"Creating examples from {len(self.documents)} documents.\")\n",
    "                self.examples = []\n",
    "                for doc_index, document in enumerate(self.documents):\n",
    "                    self.create_examples_from_document(document, doc_index)\n",
    "\n",
    "                start = time()\n",
    "                with open(cached_features_file, \"wb\") as handle:\n",
    "                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                logger.info(\n",
    "                    f\"Saving features into cached file {cached_features_file}\"\n",
    "                    f\"[took {time()-start: .3f} sec]\"\n",
    "                )\n",
    "\n",
    "    def create_examples_from_document(self, document: List[List[int]], doc_index: int):\n",
    "        \"\"\"Creates examples for a single document.\"\"\"\n",
    "        # 최대 토큰 수 = embedding size - 고정적으로 사용하는 special token 개수\n",
    "        # 여기서는 [CLS], [SEP] token이 부착되기 때문에, 2 만큼 빼줍니다.\n",
    "        # 예를 들어 embedding size가 128이면, 학습 데이터로부터 최대 126개의 token만 가져옵니다.\n",
    "        max_num_tokens = self.block_size - self.tokenizer.num_special_tokens_to_add(pair=True)\n",
    "\n",
    "        # 문장 길이의 일반화를 위해서 short_seq_prob 비율의 데이터는 짧은 길이의 문장으로 만듭니다.\n",
    "        # 예를 들어 max_num_tokens가 126이면, 2이상 126이하의 범위에서 랜덤하게 길이를 갖습니다.\n",
    "        target_seq_length = max_num_tokens\n",
    "        if random.random() < self.short_seq_prob:\n",
    "            target_seq_length = random.randint(2, max_num_tokens)\n",
    "\n",
    "        # document의 segment들을 합쳐서 chunk로 만듭니다.\n",
    "        # chunk는 seg1[SEP]seg2처럼 무조건 두 개를 합치는 것이 아니라,\n",
    "        # 126 token을 꽉 채우기 위해 seg1+seg2[SEP]seg3+seg4처럼 여러 개를 합칠 수 있습니다.\n",
    "        # 결과적으로는 tokens1[SEP]tokens2의 형태가 됩니다.\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        i = 0\n",
    "        while i < len(document):\n",
    "            segment = document[i]\n",
    "            current_chunk.append(segment)\n",
    "            current_length += len(segment)\n",
    "\n",
    "            # 마지막 segment이거나 chunk 길이가 목표 길이 이상인 경우\n",
    "            if i == len(document) - 1 or current_length >= target_seq_length:\n",
    "                # chunk를 두 부분(tokens_a, tokens_b)으로 나눕니다.\n",
    "                if current_chunk:\n",
    "                    # chunk 길이가 2 이상이면, 앞 부분을 랜덤하게 자릅니다.\n",
    "                    # 앞 부분의 길이는 2 이상 현재 chunk 길이 이하입니다.\n",
    "                    # (따라서 인덱스는 1 이상 현재 chunk 길이 - 1 이하)\n",
    "                    a_end = 1 # tokens_a의 마지막 인덱스\n",
    "                    if len(current_chunk) >= 2:\n",
    "                        a_end = random.randint(1, len(current_chunk) - 1)\n",
    "                    tokens_a = []\n",
    "                    for j in range(a_end):\n",
    "                        tokens_a.extend(current_chunk[j])\n",
    "                    \n",
    "                    # [SEP] 뒷 부분인 tokens_b를 결정합니다.\n",
    "                    # tokens_b 길이 = 전체 길이 - tokens_a 길이\n",
    "                    # tokens_b의 segments는 nsp_prob의 확률로 결정됩니다.\n",
    "                    # - nsp_prob의 확률로 랜덤하게 다른 문장을 선택합니다.\n",
    "                    # - (1-nsp_prob)의 확률로 다음 문장을 선택합니다.\n",
    "                    tokens_b = []\n",
    "                    # 랜덤하게 다른 문장을 선택하는 부분\n",
    "                    if len(current_chunk) == 1 or random.random() < self.nsp_prob:\n",
    "                        is_random_next = True\n",
    "                        target_b_length = target_seq_length - len(tokens_a)\n",
    "\n",
    "                        # 랜덤하게 선택된 doc이 원래 doc과 같지 않도록 합니다.\n",
    "                        # 보통은 한 번에 끝나겠지만, 만약을 위해 최대 10번 시도하도록 합니다.\n",
    "                        for _ in range(10):\n",
    "                            random_document_index = random.randint(0, len(self.documents) - 1)\n",
    "                            if random_document_index != doc_index:\n",
    "                                break\n",
    "                        # 선택된 랜덤 index로 document를 가져옵니다.\n",
    "                        random_document = self.documents[random_document_index]\n",
    "                        random_start = random.randint(0, len(random_document) - 1)\n",
    "                        for j in range(random_start, len(random_document)):\n",
    "                            tokens_b.extend(random_document[j])\n",
    "                            if len(tokens_b) >= target_b_length:\n",
    "                                break\n",
    "                        \n",
    "                        # 잘려서 사용되지 않은 부분은 버려지지 않도록 다시 넣어 놓습니다.\n",
    "                        num_unused_segments = len(current_chunk) - a_end\n",
    "                        i -= num_unused_segments\n",
    "                    # 실제 다음 문장을 선택하는 부분\n",
    "                    else:\n",
    "                        is_random_next = False\n",
    "                        for j in range(a_end, len(current_chunk)):\n",
    "                            tokens_b.extend(current_chunk[j])\n",
    "\n",
    "                    def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens):\n",
    "                        \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
    "                        while True:\n",
    "                            total_length = len(tokens_a) + len(tokens_b)\n",
    "                            # 종료 조건: 최대 토큰수 이하면 trunc를 종료\n",
    "                            if total_length <= max_num_tokens:\n",
    "                                break\n",
    "\n",
    "                            # tokens_a, b 중 더 긴 것을 trunc합니다.\n",
    "                            trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
    "                            assert len(trunc_tokens) >= 1\n",
    "\n",
    "                            # 랜덤하게 앞에서 혹은 뒤에서 trunc합니다.\n",
    "                            # 비율은 0.5로 하여 bias를 방지합니다.\n",
    "                            if random.random() < 0.5:\n",
    "                                del trunc_tokens[0] # 앞에서 token 하나 제거\n",
    "                            else:\n",
    "                                trunc_tokens.pop() # 뒤에서 token 하나 제거\n",
    "\n",
    "                    truncate_seq_pair(tokens_a, tokens_b, max_num_tokens)\n",
    "\n",
    "                    assert len(tokens_a) >= 1\n",
    "                    assert len(tokens_b) >= 1\n",
    "\n",
    "                    # add special tokens\n",
    "                    input_ids = self.tokenizer.build_inputs_with_special_tokens(tokens_a, tokens_b)\n",
    "                    # add token type ids, 0 for sentence a, 1 for sentence b\n",
    "                    token_type_ids = self.tokenizer.create_token_type_ids_from_sequences(tokens_a, tokens_b)\n",
    "                    \n",
    "                    # 드디어 아래 항목에 대한 데이터셋이 만들어졌습니다! :-)\n",
    "                    # 즉, segmentA[SEP]segmentB, [0, 0, .., 0, 1, 1, ..., 1], NSP 데이터가 만들어진 것입니다 :-)\n",
    "                    # 그럼 다음은.. 이 데이터에 [MASK] 를 씌워야겠죠?\n",
    "                    example = {\n",
    "                        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                        \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                        \"next_sentence_label\": torch.tensor(1 if is_random_next else 0, dtype=torch.long),\n",
    "                    }\n",
    "\n",
    "                    self.examples.append(example)\n",
    "\n",
    "                current_chunk = []\n",
    "                current_length = 0\n",
    "\n",
    "            i += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.examples[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset using small wiki corpus\n",
    "dataset = TextDatasetForNextSentencePrediction(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"/opt/ml/other/BERT_pretrain/data/wiki_20190620_small.txt\",\n",
    "    block_size=128,\n",
    "    overwrite_cache=False,\n",
    "    short_seq_prob=0.1,\n",
    "    nsp_prob=0.5,\n",
    ")\n",
    "\n",
    "# data collator\n",
    "# [MASK] 씌우는 부분은 직접 구현할 필요 없다.\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([    2,  2492,  2429,  2780,  1969,  5380,  3118,  1941,  2408,    16,\n",
      "         5498, 10307, 16245,   555,  1242,   822,  1389,  1082,   930, 16489,\n",
      "        12287,  1124,  3667,    16,  6533,  8935,  1016,  2678,  1907,    16,\n",
      "          176,   984,  4021,  1014,  8599,   729,  1167,    93,  7745,    93,\n",
      "        10414,  1006, 18366,  3486, 18888,    16,  6439,  1969,  4021,   280,\n",
      "         3362,   659,  1338,  2106,  1934, 17662,    93,   441,  1086,  2138,\n",
      "            1,  2024,  4087, 17981,    16,  2063,   498,  2736,     5, 17662,\n",
      "        12973,     5,   381,  7721,    16,  4187,  6533,   751,   544,  1030,\n",
      "         2796,  4862,  5153,  4784,   176, 11844, 15557,   656,  2784,  9396,\n",
      "         1947,  2371,  2896,  2055,    14,  9871,  6533,   751,   763,  1054,\n",
      "         6463,  5153,  2825,  3951,     3,  2830,  4531,   729, 16245, 18217,\n",
      "         2662, 17628, 13572,  2493,    14,  3952,  1917, 16011,  6533,   763,\n",
      "         2878,  6609,  1900,    16,     3]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1]), 'next_sentence_label': tensor(0)}\n"
     ]
    }
   ],
   "source": [
    "# check dataset\n",
    "for example in dataset.examples[0:1]:\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    2,     4, 11555,  ...,     4,    16,     3],\n",
      "        [    2,    14,     4,  ..., 10313,    16,     3],\n",
      "        [    2,  1984,   751,  ...,   291,   176,     3],\n",
      "        ...,\n",
      "        [    2,  4350,  2496,  ...,  3065,    16,     3],\n",
      "        [    2,  1984,  3954,  ...,  9865,    16,     3],\n",
      "        [    2,  7000,  1931,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'next_sentence_label': tensor([0, 0, 0,  ..., 1, 0, 0]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[-100, 2492, 2429,  ..., 1900, -100, -100],\n",
      "        [-100, -100, 5060,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]])}\n"
     ]
    }
   ],
   "source": [
    "# check collator: add [MASK]\n",
    "print(data_collator(dataset.examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]어는 민주당 출신 미국 39 [MASK] 대통령 이다. [MASK] 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다. 조지아 공과대학교를 졸업하였다. 그 [MASK] 해군에 들어가 전함 · 원자력 [MASK] 잠수함의 승무원으로 [MASK]. 1953년 미국 해군 대위로 예 [MASK]하였고 이후 땅콩 · 면화 [MASK] [UNK] 많은 돈을 벌었다. 그의 별 [MASK] \" 땅콩 농부 \" 로 알려졌다. 1962년 조지아 주 상원 의원 선거에서 낙선하나 그 선거가 부정선거 였음을 입증하게 되어 당선되고, 1966년 조지아 주 지사 선거에 낙선하지만 [MASK] [SEP] 대통령이 되기 전 조지아주 상원의원을 두번 연임했으며, 1971년부터 [MASK] 조지아 지사로 근무했다. [SEP]'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data_collator(dataset.examples)['input_ids'][0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mipari3\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.26<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">model_output</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ipari3/huggingface\" target=\"_blank\">https://wandb.ai/ipari3/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ipari3/huggingface/runs/afl8myek\" target=\"_blank\">https://wandb.ai/ipari3/huggingface/runs/afl8myek</a><br/>\n",
       "                Run data is saved locally in <code>/opt/ml/other/BERT_pretrain/wandb/run-20210416_043032-afl8myek</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2' max='740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/740 : < :, Epoch 0.01/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=740, training_loss=8.999630531104835, metrics={'train_runtime': 388.7286, 'train_samples_per_second': 1.904, 'total_flos': 1784424819165000.0, 'epoch': 10.0, 'init_mem_cpu_alloc_delta': 1613869056, 'init_mem_gpu_alloc_delta': 406882304, 'init_mem_cpu_peaked_delta': 275832832, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 126128128, 'train_mem_gpu_alloc_delta': 1284682240, 'train_mem_cpu_peaked_delta': 0, 'train_mem_gpu_peaked_delta': 4736489472})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model_output\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"model_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM, pipeline\n",
    "\n",
    "my_model = BertForMaskedLM.from_pretrained(\"model_output\")\n",
    "nlp_fill = pipeline('fill-mask', top_k=5, model=my_model, tokenizer=tokenizer)\n",
    "nlp_fill('이순신은 [MASK] 중기의 무신이다.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python377jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}